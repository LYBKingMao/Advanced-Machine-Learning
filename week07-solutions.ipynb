{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practical you will explore different methods of converting text into numerical vector so that it can be applied to train Neural Network models. The learning task that you will be working on today is referred to as sentiment analysis and it involves predicting whether a text represents a positive or a negative sentiment. In Task 1 we will implement a toy example using a very small dataset. In Task 2 we will use a real-world dataset containing restaurant reviews obtained from Yelp.\n",
    "\n",
    "We will be working with the Word2Vec and GloVe pre-trained word embeddings using the Gensim library. You will have to install the library (can be done via the Anaconda Navigator) and also download the Google word2vec and the GloVe pre-trained models. Please read this [Tutorial](https://machinelearningmastery.com/develop-word-embeddings-python-gensim/) before you start working on this practical. \n",
    "\n",
    "NOTE: When you are loading the pre-trained word2vec or glove embeddings you can use argument $limit=50000$ in order to speed up the process. This will get only the embeddings of the 50000 most popular words. Otherwise it may take quite a while to load the models into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "In this task you will develop a model which predicts whether a short text contains positive or negative emotions. The train and test datasets are presented below. It is a 2-class classification problem, when 1 - stands for positive and 0 - stands for negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "from numpy.random import seed\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ['Well done!',\n",
    "        'Good work',\n",
    "        'Great effort',\n",
    "        'nice work',\n",
    "        'Excellent!',\n",
    "        'Weak',\n",
    "        'Poor effort!',\n",
    "        'not good',\n",
    "        'poor work',\n",
    "        'Could have done better.']\n",
    "\n",
    "labels_train = np.array([1,1,1,1,1,0,0,0,0,0])\n",
    "\n",
    "test = ['Amazing job!',\n",
    "        'Fantastic work',\n",
    "        'Good effort',\n",
    "        'Could not have done better',\n",
    "        'not great',\n",
    "        'poor job',\n",
    "        'poor job',\n",
    "        'very weak',]\n",
    "\n",
    "labels_test = np.array([1,1,1,1,0,0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**T1.1 Obtaining Bag of Words (BOW) binary representation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the vectorizer\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "\n",
    "# Converting the train data into vectors\n",
    "data_train = vectorizer.fit_transform(train).toarray()\n",
    "\n",
    "#Converting the test data into vectors\n",
    "data_test = vectorizer.transform(test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 1 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 1]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 1]\n",
      " [1 1 1 0 0 0 0 1 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**T1.2 Training and evaluating NN model with the binary BOW representation**\n",
    "\n",
    "Now you can use your vector representation of the train/test data and the labels in order to train and evaluate a simple NN model. Implement a simple MLP model and train/test it using the obtained vector representation of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel(n_features):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(5, input_dim = n_features, activation='relu', kernel_initializer='random_uniform', bias_initializer='zeros'))\n",
    "    model.add(Dense(2, activation='sigmoid', kernel_initializer='random_uniform', bias_initializer='zeros'))\n",
    "    opt = optimizers.Adam(lr=0.05, beta_1=0.9, beta_2=0.999)\n",
    "    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 2ms/step\n",
      "Accuracy on test dataset: 0.75\n",
      "Loss:  0.9601109623908997\n"
     ]
    }
   ],
   "source": [
    "model = getModel(len(vectorizer.vocabulary_))\n",
    "model.fit(data_train, labels_train, epochs=20, batch_size=data_train.shape[0], verbose=0)\n",
    "\n",
    "#Evaluating the model on the test data\n",
    "loss, acc = model.evaluate(data_test, labels_test)\n",
    "print('Accuracy on test dataset: %.2f' % acc)\n",
    "print('Loss: ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**T1.3 BOW - word count representation**\n",
    "\n",
    "Do the same as above, but instead of binary BOW use the word count BOW representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 2ms/step\n",
      "Accuracy on test dataset: 0.75\n",
      "Loss:  1.1510188579559326\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Converting the train data into vectors\n",
    "data_train = vectorizer.fit_transform(train).toarray()\n",
    "\n",
    "#Converting the test data into vectors\n",
    "data_test = vectorizer.transform(test).toarray()\n",
    "\n",
    "model = getModel(len(vectorizer.vocabulary_))\n",
    "model.fit(data_train, labels_train, epochs=20, batch_size=data_train.shape[0], verbose=0)\n",
    "\n",
    "#Evaluating the model on the test data\n",
    "loss, acc = model.evaluate(data_test, labels_test)\n",
    "print('Accuracy on test dataset: %.2f' % acc)\n",
    "print('Loss: ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**T1.4 BOW - TF-IDF representation**\n",
    "\n",
    "This time you should use the TFIDF version of the BOW method to convert the text into vectors and apply it to train and test your NN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 2ms/step\n",
      "Accuracy on test dataset: 0.75\n",
      "Loss:  2.041787624359131\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# create the vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Converting the train data into vectors\n",
    "data_train = vectorizer.fit_transform(train).toarray()\n",
    "\n",
    "#Converting the test data into vectors\n",
    "data_test = vectorizer.transform(test).toarray()\n",
    "\n",
    "model = getModel(len(vectorizer.vocabulary_))\n",
    "model.fit(data_train, labels_train, epochs=50, batch_size=data_train.shape[0], verbose=0)\n",
    "\n",
    "#Evaluating the model on the test data\n",
    "loss, acc = model.evaluate(data_test, labels_test)\n",
    "print('Accuracy on test dataset: %.2f' % acc)\n",
    "print('Loss: ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**T1.5 Words Embeddings - word2vec**\n",
    "\n",
    "In this task we will use the pre-trained embeddings from the word2vec model.\n",
    "\n",
    "First, following the [Tutorial](https://machinelearningmastery.com/develop-word-embeddings-python-gensim/) you should download the word2vec model. You can then load it into memory as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#file = 'D:\\PycharmProjects\\RecordLinkage\\Embedings Files\\GoogleNews-vectors-negative300.bin'\n",
    "file = 'GoogleNews-vectors-negative300.bin'\n",
    "word2vec = KeyedVectors.load_word2vec_format(file, binary=True, limit=50000)\n",
    "word2vec_vectors = word2vec.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most straightforward approach to obtaining vector representation for a sentence/document is to obtained a vector representation for each of the individual words within the document and average them. There are multiple ways of implementing it, below is just one of them.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting text data into vectors\n",
    "def getWord2Vec(list):\n",
    "    vectors = []\n",
    "    for row in list:\n",
    "        tokens = [w.lower() for w in re.sub(r'[^\\w\\s]','',row).split(' ')]\n",
    "        temp = []\n",
    "        for token in tokens:\n",
    "            if token in word2vec_vectors:\n",
    "                temp.append(word2vec[token])\n",
    "        vectors.append(np.mean(temp, axis=0))\n",
    "    return np.asarray(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use the getWord2Vec method to obtain vector representation of each of the train and test record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.59179688e-02  1.31835938e-01 -9.08203125e-02  6.95800781e-02\n",
      " -1.24023438e-01 -8.37402344e-02  1.89285278e-02  3.60107422e-02\n",
      "  1.18896484e-01 -6.66503906e-02 -1.13525391e-02 -1.23168945e-01\n",
      "  8.36791992e-02 -1.07421875e-02 -1.22070312e-03  2.08190918e-01\n",
      "  9.72290039e-02  9.70458984e-02  6.00585938e-02 -9.06066895e-02\n",
      " -4.27246094e-02  1.30859375e-01 -2.83203125e-02 -9.35058594e-02\n",
      "  1.51916504e-01 -6.61010742e-02 -1.25488281e-01 -1.82617188e-01\n",
      " -1.06689453e-01  2.24609375e-02 -3.52172852e-02  4.58984375e-02\n",
      "  1.19873047e-01 -1.25488281e-01  6.28356934e-02  9.20410156e-02\n",
      "  1.75781250e-02 -1.19628906e-02  8.23974609e-02 -1.58691406e-02\n",
      "  2.22656250e-01  2.35595703e-02  3.91235352e-02  1.72729492e-02\n",
      " -1.81274414e-02 -3.73535156e-02 -1.93359375e-01 -4.23583984e-02\n",
      "  3.58886719e-02 -4.31976318e-02  6.81762695e-02  9.13085938e-02\n",
      " -4.59594727e-02 -5.61523438e-02 -5.56640625e-02  1.68457031e-02\n",
      " -1.12060547e-01 -4.46777344e-02  1.37451172e-01 -4.23812866e-02\n",
      " -7.76062012e-02  1.87988281e-01 -1.38793945e-01 -2.10937500e-01\n",
      "  3.19519043e-02  3.83300781e-02 -1.47338867e-01 -3.68041992e-02\n",
      " -1.87500000e-01 -3.07617188e-02  9.57031250e-02  7.14721680e-02\n",
      "  2.16552734e-01 -7.93457031e-03 -2.23632812e-01 -7.54394531e-02\n",
      "  6.15234375e-02  1.24267578e-01  1.94091797e-02  7.75756836e-02\n",
      " -1.08398438e-01  7.03125000e-02 -6.22558594e-03  9.76562500e-03\n",
      "  5.49316406e-03 -1.01379395e-01 -5.66406250e-02  1.40136719e-01\n",
      " -2.56347656e-02  9.27734375e-02  5.93261719e-02  1.78222656e-01\n",
      " -1.95312500e-02  1.53808594e-02  1.29638672e-01 -1.21109009e-01\n",
      " -4.82177734e-02  1.31225586e-02 -6.83593750e-03 -5.20629883e-02\n",
      " -9.75341797e-02 -5.62744141e-02 -1.20849609e-01  8.58154297e-02\n",
      "  5.78613281e-02 -1.13525391e-01 -1.57836914e-01 -9.70153809e-02\n",
      "  2.77404785e-02 -2.75878906e-02  3.17382812e-03  7.98339844e-02\n",
      "  3.24707031e-02  5.77392578e-02  3.02734375e-02  4.75463867e-02\n",
      "  1.41845703e-01 -2.72827148e-02  7.24334717e-02 -7.04956055e-03\n",
      " -5.18188477e-02  7.80029297e-02 -4.08935547e-02  1.43066406e-01\n",
      " -1.47583008e-01  4.52880859e-02 -9.74121094e-02 -5.55572510e-02\n",
      "  1.95312500e-02 -7.93457031e-02 -1.45629883e-01 -1.40869141e-01\n",
      " -9.98535156e-02 -4.63256836e-02 -1.47583008e-01 -7.41882324e-02\n",
      "  1.24511719e-01  1.95800781e-01  3.00292969e-02  6.90917969e-02\n",
      " -1.67770386e-02  5.95703125e-02  1.41601562e-02 -3.72314453e-02\n",
      "  5.56640625e-02 -2.13623047e-03 -1.21093750e-01  5.13916016e-02\n",
      "  2.40478516e-02 -4.49218750e-02  4.88586426e-02  1.68457031e-01\n",
      " -9.28955078e-02  1.53320312e-01 -2.88772583e-02  2.38342285e-02\n",
      " -7.88574219e-02 -1.42578125e-01 -3.44848633e-02  7.31811523e-02\n",
      "  1.13037109e-01  2.68554688e-01 -1.00097656e-02  1.42578125e-01\n",
      "  8.30078125e-02 -2.13867188e-01 -8.20312500e-02 -1.11816406e-01\n",
      "  8.66699219e-02 -2.67333984e-02 -1.21582031e-01 -3.58886719e-02\n",
      "  2.20947266e-02 -9.25292969e-02  7.27539062e-02  1.10595703e-01\n",
      "  9.25292969e-02 -8.29162598e-02  8.30078125e-02  4.93164062e-02\n",
      " -1.05957031e-01 -8.39233398e-02 -4.61425781e-02 -6.66503906e-02\n",
      " -7.78808594e-02  1.30126953e-01  1.61132812e-02  1.90429688e-02\n",
      "  3.67584229e-02  1.33056641e-01 -2.38037109e-02  1.19873047e-01\n",
      "  1.46728516e-01 -9.99755859e-02 -7.08007812e-02 -1.20849609e-01\n",
      "  2.88085938e-02  6.10351562e-02  9.98535156e-02 -1.51367188e-01\n",
      "  2.36816406e-02  1.55578613e-01 -5.93872070e-02  4.88281250e-03\n",
      " -1.22070312e-04  7.81250000e-03 -9.01422501e-02 -1.60400391e-01\n",
      "  3.15093994e-03  6.59179688e-02 -1.09863281e-02  1.19430542e-01\n",
      " -1.10351562e-01 -4.40673828e-02  5.98144531e-03 -6.73828125e-02\n",
      "  6.70776367e-02  4.02984619e-02  9.52148438e-03  5.76171875e-02\n",
      " -2.60009766e-02 -1.72851562e-01 -1.42822266e-01 -2.36816406e-02\n",
      "  6.12487793e-02 -2.09960938e-02 -3.35388184e-02 -3.61328125e-02\n",
      " -1.90429688e-02 -6.95800781e-03  8.11767578e-03 -2.14843750e-01\n",
      "  1.10351562e-01 -1.55029297e-02 -4.84619141e-02 -2.09960938e-02\n",
      " -1.28784180e-01 -9.22851562e-02  1.38427734e-01 -7.71484375e-02\n",
      "  2.55126953e-02  9.76562500e-02  5.37109375e-03 -7.68127441e-02\n",
      " -3.88183594e-02 -4.34265137e-02  3.41796875e-03 -1.94091797e-02\n",
      " -1.19415283e-01 -1.32324219e-01 -6.10351562e-04  1.68945312e-01\n",
      "  8.83789062e-02  3.85742188e-02  1.47949219e-01 -3.64379883e-02\n",
      " -1.13037109e-01 -1.29699707e-02 -1.66625977e-02 -2.99804688e-01\n",
      "  8.54492188e-03  6.22558594e-03  2.97851562e-02 -3.91845703e-02\n",
      "  8.79459381e-02  1.87500000e-01 -1.92871094e-02  1.22680664e-02\n",
      "  3.93066406e-02 -3.85742188e-02  9.54589844e-02 -3.34472656e-02\n",
      "  1.76086426e-02  1.66992188e-01  3.19824219e-02 -1.82861328e-01\n",
      "  7.27539062e-02 -2.01171875e-01 -1.12304688e-02 -2.78320312e-02\n",
      "  6.88629150e-02  7.59277344e-02  4.75463867e-02  1.26708984e-01\n",
      "  6.19792938e-02 -2.73275375e-02 -9.27734375e-03 -5.89294434e-02\n",
      "  9.44824219e-02  1.44531250e-01 -1.23046875e-01 -4.29687500e-02\n",
      " -2.30468750e-01  1.39404297e-01 -1.28128052e-01  1.67968750e-01\n",
      "  8.14819336e-02 -3.60107422e-02  2.28691101e-02 -1.63085938e-01]\n",
      "(10, 300)\n",
      "(8, 300)\n"
     ]
    }
   ],
   "source": [
    "#Converting train and text data into vectors\n",
    "data_train = getWord2Vec(train)\n",
    "print(data_train[0])\n",
    "data_test = getWord2Vec(test)     \n",
    "print(data_train.shape)\n",
    "print(np.asarray(data_test).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 17ms/step\n",
      "Accuracy on test dataset: 0.88\n",
      "Loss:  1.7167880535125732\n"
     ]
    }
   ],
   "source": [
    "model = getModel(300)\n",
    "model.fit(data_train, labels_train, epochs=50, batch_size=data_train.shape[0], verbose=0)\n",
    "\n",
    "#Evaluating the model on the test data\n",
    "loss, acc = model.evaluate(data_test, labels_test)\n",
    "print('Accuracy on test dataset: %.2f' % acc)\n",
    "print('Loss: ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**T1.6 Words Embeddings - GloVe**\n",
    "\n",
    "In this task you should do the same as above but instead of Word2Ves use the GloVe model. You can use the GloVe model with Gensim library as presented in the [Tutorial](https://machinelearningmastery.com/develop-word-embeddings-python-gensim/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file = 'D:\\PycharmProjects\\RecordLinkage\\Embedings Files\\Glove files\\glove.6B.300d.txt'\n",
    "word2vec_output_file = 'D:\\PycharmProjects\\RecordLinkage\\Embedings Files\\Glove files\\glove.6B.300d_1.txt'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\3049848\\AppData\\Local\\Anaconda\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# load the Stanford GloVe model\n",
    "filename = '/Users/annajurek/Documents/Queens/word embedding/glove.6B/glove.6B.50d.txt.word2vec'\n",
    "#filename = 'D:\\PycharmProjects\\RecordLinkage\\Embedings Files\\Glove files\\glove.6B.300d_1.txt'\n",
    "glove = KeyedVectors.load_word2vec_format(filename, binary=False, limit=50000)\n",
    "glove_vectors = glove.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting text data into vectors\n",
    "def getGlove(list):\n",
    "    vectors = []\n",
    "    for row in list:\n",
    "        tokens = [w.lower() for w in re.sub(r'[^\\w\\s]','',row).split(' ')]\n",
    "        temp = []\n",
    "        for token in tokens:\n",
    "            if token in glove_vectors:\n",
    "                temp.append(glove[token])\n",
    "        vectors.append(np.mean(temp, axis=0))\n",
    "    return np.asarray(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 300)\n",
      "(8, 300)\n"
     ]
    }
   ],
   "source": [
    "#Converting train and text data into vectors\n",
    "data_train = getGlove(train)\n",
    "data_test = getGlove(test)     \n",
    "print(data_train.shape)\n",
    "print(np.asarray(data_test).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 20ms/step\n",
      "Accuracy on test dataset: 0.75\n",
      "Loss:  0.7757281064987183\n"
     ]
    }
   ],
   "source": [
    "model = getModel(300)\n",
    "model.fit(data_train, labels_train, epochs=50, batch_size=data_train.shape[0], verbose=0)\n",
    "\n",
    "#Evaluating the model on the test data\n",
    "loss, acc = model.evaluate(data_test, labels_test)\n",
    "print('Accuracy on test dataset: %.2f' % acc)\n",
    "print('Loss: ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "In this task we will repeat the same steps as in Task one but this time we will use a real-world dataset with reviews of restaurants obtained from Yelp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('yelp_reviews.csv',encoding = \"ISO-8859-1\")\n",
    "\n",
    "#select input and output variables\n",
    "data = df.values[:,0]\n",
    "labels = df.values[:,1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**T2.1** Split the data into train and test sets. Use any test size.\n",
    "\n",
    "**T2.2** Obtained vector representation of train and test data using Binary BOW, TFIDF, Word2Vec and Glove\n",
    "\n",
    "**T2.3** Implement a MLP and evaluate it with each of the data representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data, labels,test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498/498 [==============================] - 0s 221us/step\n",
      "BOW Accuracy on test dataset: 0.76\n",
      "Loss:  1.6894555771685986\n",
      "498/498 [==============================] - 0s 263us/step\n",
      "TFIDF Accuracy on test dataset: 0.76\n",
      "Loss:  1.739709367713775\n",
      "498/498 [==============================] - 0s 279us/step\n",
      "W2V Accuracy on test dataset: 0.83\n",
      "Loss:  1.4289307043734325\n",
      "498/498 [==============================] - 0s 273us/step\n",
      "Glove Accuracy on test dataset: 0.80\n",
      "Loss:  1.57802675383158\n"
     ]
    }
   ],
   "source": [
    "#Bag of Words - Binary\n",
    "bow_train = vectorizer.fit_transform(x_train).toarray()\n",
    "bow_test = vectorizer.transform(x_test).toarray()\n",
    "bow_model = getModel1(1, [50], bow_train, y_train)\n",
    "loss, acc = bow_model.evaluate(bow_test, y_test)\n",
    "print('BOW Accuracy on test dataset: %.2f' % acc)\n",
    "print('Loss: ', loss)\n",
    "\n",
    "#Bag of words - TFIDF\n",
    "tfidf_train = vectorizer.fit_transform(x_train).toarray()\n",
    "tfidf_test = vectorizer.transform(x_test).toarray()\n",
    "tfidf_model = getModel1(1, [50], tfidf_train, y_train)\n",
    "loss, acc = tfidf_model.evaluate(tfidf_test, y_test)\n",
    "print('TFIDF Accuracy on test dataset: %.2f' % acc)\n",
    "print('Loss: ', loss)\n",
    "\n",
    "#Embeddings - W2V\n",
    "w2v_train = getWord2Vec(x_train)\n",
    "w2v_test = getWord2Vec(x_test) \n",
    "w2v_model = getModel1(1, [50], w2v_train, y_train)\n",
    "loss, acc = w2v_model.evaluate(w2v_test, y_test)\n",
    "print('W2V Accuracy on test dataset: %.2f' % acc)\n",
    "print('Loss: ', loss)\n",
    "\n",
    "#Embeddings - GloVe\n",
    "glove_train = getGlove(x_train)\n",
    "glove_test = getGlove(x_test)\n",
    "glove_model = getModel1(1, [50], glove_train, y_train)\n",
    "loss, acc = glove_model.evaluate(glove_test, y_test)\n",
    "print('Glove Accuracy on test dataset: %.2f' % acc)\n",
    "print('Loss: ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel1(h_layers_no, neurons_no, data_train, labels_train):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons_no[0], input_dim = data_train.shape[1], activation='relu', kernel_initializer='random_uniform', bias_initializer='zeros'))\n",
    "    for l in range(h_layers_no-1):\n",
    "        model.add(Dense(neurons_no[l], activation='relu', kernel_initializer='random_uniform', bias_initializer='zeros'))\n",
    "    model.add(Dense(2, activation='sigmoid', kernel_initializer='random_uniform', bias_initializer='zeros'))\n",
    "    opt = optimizers.Adam(lr=0.05, beta_1=0.9, beta_2=0.999)\n",
    "    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(data_train, labels_train, epochs=100, batch_size=32, verbose=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# list of text documents\n",
    "text = [\"i am feeling very very happy.\",\n",
    "       \"I am not well today.\",\n",
    "       \"I want to be happy.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary Scoring**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 4, 'am': 0, 'feeling': 2, 'very': 8, 'happy': 3, 'not': 5, 'well': 10, 'today': 7, 'want': 9, 'to': 6, 'be': 1}\n",
      "[[1 0 1 1 1 0 0 0 1 0 0]\n",
      " [1 0 0 0 1 1 0 1 0 0 1]\n",
      " [0 1 0 1 1 0 1 0 0 1 0]]\n",
      "  (0, 0)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 8)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 10)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 9)\t1\n",
      "(3, 11)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# create the transform\n",
    "vectorizer = CountVectorizer(binary=True, token_pattern = r\"(?u)\\b\\w+\\b\")#regular expression\n",
    "\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "\n",
    "# encode documents\n",
    "vectors = vectorizer.transform(text)\n",
    "print(vectors.toarray())\n",
    "print(vectors)\n",
    "\n",
    "# summarize encoded vector\n",
    "print(vectors.shape)\n",
    "print(type(vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Counts Scoring**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 1 1 0 0 0 2 0 0]\n",
      " [1 0 0 0 1 1 0 1 0 0 1]\n",
      " [0 1 0 1 1 0 1 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# create the transform\n",
    "vectorizer = CountVectorizer(token_pattern = r\"(?u)\\b\\w+\\b\")\n",
    "\n",
    "# tokenize and build vocab\n",
    "vectors = vectorizer.fit_transform(text)\n",
    "\n",
    "print(vectors.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Frequency Scoring**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.17 0.   0.17 0.17 0.17 0.   0.   0.   0.33 0.   0.  ]\n",
      " [0.2  0.   0.   0.   0.2  0.2  0.   0.2  0.   0.   0.2 ]\n",
      " [0.   0.2  0.   0.2  0.2  0.   0.2  0.   0.   0.2  0.  ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer(token_pattern = r\"(?u)\\b\\w+\\b\")\n",
    "\n",
    "vectors = vectorizer.fit_transform(text)\n",
    "vectors = vectors/vectors.sum(axis=1)\n",
    "np.set_printoptions(precision=2)\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Additional Features: Removing stop words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feeling': 0, 'happy': 1, 'today': 2, 'want': 3}\n",
      "[[1 1 0 0]\n",
      " [0 0 1 0]\n",
      " [0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# create the transform\n",
    "vectorizer = CountVectorizer(binary=True, token_pattern = r\"(?u)\\b\\w+\\b\", stop_words='english')\n",
    "\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "\n",
    "# encode documents\n",
    "vectors = vectorizer.transform(text)\n",
    "print(vectors.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TFIDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 10)\n",
      "[[0.34385143 0.         0.45212331 0.68770286 0.         0.\n",
      "  0.         0.45212331 0.         0.        ]\n",
      " [0.40204024 0.         0.         0.         0.52863461 0.\n",
      "  0.52863461 0.         0.         0.52863461]\n",
      " [0.         0.52863461 0.         0.40204024 0.         0.52863461\n",
      "  0.         0.         0.52863461 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "\n",
    "# encode document\n",
    "vectors = vectorizer.transform(text)\n",
    "\n",
    "# summarize encoded vector\n",
    "print(vectors.shape)\n",
    "print(vectors.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

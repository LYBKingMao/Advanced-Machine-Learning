{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "lb=preprocessing.LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit label binarizer by classes \n",
    "lb.fit([0,1,2,3,4,5,6,7,8,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-processing\n",
    "data,label=datasets.load_digits(return_X_y=True);\n",
    "data = preprocessing.MinMaxScaler().fit_transform(data)\n",
    "label=label.reshape(len(label),1)\n",
    "\n",
    "#classes included in dataset\n",
    "labels = [0,1,2,3,4,5,6,7,8,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set number of fold, random seeds minibatch size and other variables\n",
    "kf=KFold(n_splits=10,shuffle=True)\n",
    "np.random.seed(0)\n",
    "minibatch_size=64\n",
    "l=0.1\n",
    "epochs=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relu function\n",
    "def ReLu(v):\n",
    "    return np.maximum(0,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#derivative of relu funtion\n",
    "def ReLu_derivative(v):\n",
    "    return np.where(v<=0,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LReLU(x):\n",
    "    return np.where(x > 0, x, x * 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dLReLU(x):\n",
    "    return np.where(x > 0, 1, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax funtion\n",
    "def softmax(X):\n",
    "    exps = np.exp(X)\n",
    "    return exps / np.sum(exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert softmax output to single label\n",
    "def convert(arr):\n",
    "    arr_size = arr.shape[1]\n",
    "    arr_max = np.argmax(arr, axis=1)\n",
    "    return arr_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error of fold 1 epoch 0 is: 1.6461951655224727\n",
      "Training error of fold 1 epoch 100 is: 0.002858654294997293\n",
      "Training error of fold 1 epoch 200 is: 0.001489406725692198\n",
      "Training error of fold 1 epoch 300 is: 0.0010081069055477322\n",
      "Training error of fold 1 epoch 400 is: 0.0007805021792463138\n",
      "Training error of fold 1 epoch 500 is: 0.0006396413825392362\n",
      "Training error of fold 1 epoch 600 is: 0.0005388994764754972\n",
      "Training error of fold 1 epoch 700 is: 0.00046320536109123586\n",
      "Training error of fold 1 epoch 800 is: 0.00040599441851739144\n",
      "Training error of fold 1 epoch 900 is: 0.0003616241174174815\n",
      "Fold 1 's f-score is: 0.9554067281378769\n",
      "Training error of fold 2 epoch 0 is: 1.5562704189306145\n",
      "Training error of fold 2 epoch 100 is: 0.0017057487087113593\n",
      "Training error of fold 2 epoch 200 is: 0.0011277515990566964\n",
      "Training error of fold 2 epoch 300 is: 0.0008824839248875199\n",
      "Training error of fold 2 epoch 400 is: 0.0007116376568054429\n",
      "Training error of fold 2 epoch 500 is: 0.0005962913678796143\n",
      "Training error of fold 2 epoch 600 is: 0.0005142036168152574\n",
      "Training error of fold 2 epoch 700 is: 0.00045373308313971397\n",
      "Training error of fold 2 epoch 800 is: 0.000406572112963201\n",
      "Training error of fold 2 epoch 900 is: 0.0003671343406905429\n",
      "Fold 2 's f-score is: 0.9707755455985196\n",
      "Training error of fold 3 epoch 0 is: 3.7688393366772543\n",
      "Training error of fold 3 epoch 100 is: 0.0028658476879366165\n",
      "Training error of fold 3 epoch 200 is: 0.0016988443954939808\n",
      "Training error of fold 3 epoch 300 is: 0.0012147431955868897\n",
      "Training error of fold 3 epoch 400 is: 0.0009440032341664077\n",
      "Training error of fold 3 epoch 500 is: 0.0007763071698883251\n",
      "Training error of fold 3 epoch 600 is: 0.0006514270114221555\n",
      "Training error of fold 3 epoch 700 is: 0.0005568729606075734\n",
      "Training error of fold 3 epoch 800 is: 0.000484195218124382\n",
      "Training error of fold 3 epoch 900 is: 0.0004293261437146281\n",
      "Fold 3 's f-score is: 0.961050405362991\n",
      "Training error of fold 4 epoch 0 is: 0.8507083432403724\n",
      "Training error of fold 4 epoch 100 is: 0.0033805378437942184\n",
      "Training error of fold 4 epoch 200 is: 0.0015260478609168979\n",
      "Training error of fold 4 epoch 300 is: 0.001000520977209594\n",
      "Training error of fold 4 epoch 400 is: 0.000745977067322209\n",
      "Training error of fold 4 epoch 500 is: 0.0005971922728001807\n",
      "Training error of fold 4 epoch 600 is: 0.0004979257783294425\n",
      "Training error of fold 4 epoch 700 is: 0.0004261586299062092\n",
      "Training error of fold 4 epoch 800 is: 0.00037298621827737015\n",
      "Training error of fold 4 epoch 900 is: 0.00033169810740006015\n",
      "Fold 4 's f-score is: 0.969052748326942\n",
      "Training error of fold 5 epoch 0 is: 1.4777155953889656\n",
      "Training error of fold 5 epoch 100 is: 0.004292596815662346\n",
      "Training error of fold 5 epoch 200 is: 0.0018326711741602475\n",
      "Training error of fold 5 epoch 300 is: 0.0011341752545380806\n",
      "Training error of fold 5 epoch 400 is: 0.0008209761923551435\n",
      "Training error of fold 5 epoch 500 is: 0.000636762710199132\n",
      "Training error of fold 5 epoch 600 is: 0.0005251637075290301\n",
      "Training error of fold 5 epoch 700 is: 0.0004476348151290684\n",
      "Training error of fold 5 epoch 800 is: 0.0003855612062669457\n",
      "Training error of fold 5 epoch 900 is: 0.00033911673853398064\n",
      "Fold 5 's f-score is: 0.981487827928506\n",
      "Training error of fold 6 epoch 0 is: 6.03851427792516\n",
      "Training error of fold 6 epoch 100 is: 0.0016545033496727895\n",
      "Training error of fold 6 epoch 200 is: 0.0010570028405821673\n",
      "Training error of fold 6 epoch 300 is: 0.000799594531227669\n",
      "Training error of fold 6 epoch 400 is: 0.0006420335367195373\n",
      "Training error of fold 6 epoch 500 is: 0.0005323135807299704\n",
      "Training error of fold 6 epoch 600 is: 0.0004522402554147024\n",
      "Training error of fold 6 epoch 700 is: 0.000391726626118716\n",
      "Training error of fold 6 epoch 800 is: 0.0003425512729839174\n",
      "Training error of fold 6 epoch 900 is: 0.0003046622365982035\n",
      "Fold 6 's f-score is: 0.9639080254743406\n",
      "Training error of fold 7 epoch 0 is: 3.3716800284166406\n",
      "Training error of fold 7 epoch 100 is: 0.0026647876629743634\n",
      "Training error of fold 7 epoch 200 is: 0.001436646162778665\n",
      "Training error of fold 7 epoch 300 is: 0.0009975219278382365\n",
      "Training error of fold 7 epoch 400 is: 0.0007788451584273651\n",
      "Training error of fold 7 epoch 500 is: 0.0006385823997711061\n",
      "Training error of fold 7 epoch 600 is: 0.000536057205279217\n",
      "Training error of fold 7 epoch 700 is: 0.0004664203303180622\n",
      "Training error of fold 7 epoch 800 is: 0.00041328132070761773\n",
      "Training error of fold 7 epoch 900 is: 0.0003718616019056138\n",
      "Fold 7 's f-score is: 0.9721272362698509\n",
      "Training error of fold 8 epoch 0 is: 5.156400358314219\n",
      "Training error of fold 8 epoch 100 is: 0.005235175755258183\n",
      "Training error of fold 8 epoch 200 is: 0.002204078091767672\n",
      "Training error of fold 8 epoch 300 is: 0.0013788292840086404\n",
      "Training error of fold 8 epoch 400 is: 0.0010120517576901443\n",
      "Training error of fold 8 epoch 500 is: 0.0007970919478558127\n",
      "Training error of fold 8 epoch 600 is: 0.0006540850988140584\n",
      "Training error of fold 8 epoch 700 is: 0.0005533160584644745\n",
      "Training error of fold 8 epoch 800 is: 0.0004801023115498065\n",
      "Training error of fold 8 epoch 900 is: 0.00042773217830638176\n",
      "Fold 8 's f-score is: 0.9754139433551199\n",
      "Training error of fold 9 epoch 0 is: 5.058983961046643\n",
      "Training error of fold 9 epoch 100 is: 0.0006215668793170897\n",
      "Training error of fold 9 epoch 200 is: 0.0003059918577074676\n",
      "Training error of fold 9 epoch 300 is: 0.00024797996908001383\n",
      "Training error of fold 9 epoch 400 is: 0.0002153309809525118\n",
      "Training error of fold 9 epoch 500 is: 0.000190754972686499\n",
      "Training error of fold 9 epoch 600 is: 0.00017079906446629514\n",
      "Training error of fold 9 epoch 700 is: 0.00015279563082228298\n",
      "Training error of fold 9 epoch 800 is: 0.000138317459067952\n",
      "Training error of fold 9 epoch 900 is: 0.00012773961102606902\n",
      "Fold 9 's f-score is: 0.961835754089315\n",
      "Training error of fold 10 epoch 0 is: 2.0230358844433556\n",
      "Training error of fold 10 epoch 100 is: 0.0035481425676060618\n",
      "Training error of fold 10 epoch 200 is: 0.0020966805652707417\n",
      "Training error of fold 10 epoch 300 is: 0.0014510105981884926\n",
      "Training error of fold 10 epoch 400 is: 0.001105470830587754\n",
      "Training error of fold 10 epoch 500 is: 0.0008745624882458182\n",
      "Training error of fold 10 epoch 600 is: 0.0007286022957835345\n",
      "Training error of fold 10 epoch 700 is: 0.0006208341456200705\n",
      "Training error of fold 10 epoch 800 is: 0.0005396822516912759\n",
      "Training error of fold 10 epoch 900 is: 0.00047823520558798634\n",
      "Fold 10 's f-score is: 0.981638382568615\n",
      "Average F-score across 10 folds is:  0.9692696597112077\n"
     ]
    }
   ],
   "source": [
    "#create an array to store each fold's f-score, create a varible to visualize current progress\n",
    "f1s=[]\n",
    "fold=1;\n",
    "\n",
    "#cross validation split\n",
    "for train_index,test_index in kf.split(data):\n",
    "    w = np.random.uniform(-1,1,[data.shape[1],128])        #10 neurons for hidden layer\n",
    "    b = np.zeros([1,128])                                       #number of bias equal to number of neurons in hidden layer\n",
    "    w2 = np.random.uniform(-1,1,[128,10])                       #10 output neurons for 10 classes\n",
    "    b2 = np.zeros([1,10])\n",
    "    \n",
    "    #epoch split\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        #minibatch split\n",
    "        for i in range(0,data[train_index].shape[0],minibatch_size):\n",
    "            x_train=data[train_index]\n",
    "            y_train=label[train_index]\n",
    "            x_mini=x_train[i:i+minibatch_size,:]\n",
    "            y_mini=y_train[i:i+minibatch_size,:]\n",
    "        \n",
    "            #forward pass\n",
    "            in_=x_mini@w+b\n",
    "            out1=ReLu(in_)\n",
    "            in2=out1@w2+b2\n",
    "            out2=np.zeros([x_mini.shape[0],10])\n",
    "            for j in range(x_mini.shape[0]):\n",
    "                out2[j,:]=softmax(in2[j,:])\n",
    "        \n",
    "            #one-hot label\n",
    "            oh_label=lb.transform(y_mini)\n",
    "            \n",
    "            #training error\n",
    "            train_error=metrics.log_loss(y_mini,out2,labels=labels)\n",
    "        \n",
    "            #Back propergation layer2\n",
    "            dEdIn2=np.zeros([x_mini.shape[0],10]) \n",
    "            for j in range(x_mini.shape[0]):\n",
    "                dEdIn2[j,:]=out2[j,:]-oh_label[j,:]\n",
    "            dIn2_dW2 = out1\n",
    "            dEdW2 = (1/x_mini.shape[0])*out1.T@dEdIn2\n",
    "            dEdB2 = (1/x_mini.shape[0])*np.ones([1,len(x_mini)])@dEdIn2\n",
    "            \n",
    "            #update layer2's weights and bias\n",
    "            w2 -= l*dEdW2\n",
    "            b2 -= l*dEdB2\n",
    "\n",
    "            #Back propergation layer1\n",
    "            dEdOut1 = dEdIn2 @ w2.T\n",
    "            dOut1dIn1 = ReLu_derivative(in_)\n",
    "            dEdIn1 = dEdOut1*dOut1dIn1\n",
    "            dIn1dW = x_mini\n",
    "            dEdW = (1/x_mini.shape[0])*dIn1dW.T@((dEdIn2@w2.T)*dOut1dIn1)\n",
    "            dEdB = (1/x_mini.shape[0])*np.ones([len(x_mini)])@((dEdIn2@w2.T)*dOut1dIn1)\n",
    "\n",
    "            #update layer1's weights and bias\n",
    "            w -= l*dEdW\n",
    "            b -= l*dEdB\n",
    "        \n",
    "        #print output for each 100 epoch\n",
    "        if(epoch%100==0):\n",
    "            print(\"Training error of fold\",fold,\"epoch\",epoch,\"is:\",train_error)\n",
    "            \n",
    "    #predict test dataset and convert result to label         \n",
    "    in22=(ReLu(data[test_index]@w+b)@w2)+b2\n",
    "    out22=np.zeros([data[test_index].shape[0],10])\n",
    "    for j in range(data[test_index].shape[0]):\n",
    "        out22[j,:]=softmax(in22[j,:])\n",
    "    prediction=convert(out22)\n",
    "    \n",
    "    #calculate each fold's f-score\n",
    "    f1=f1_score(y_true=label[test_index], y_pred=prediction, labels=labels,average='macro')\n",
    "    print(\"Fold\",fold,\"'s f-score is:\",f1)\n",
    "    f1s.append(f1)\n",
    "    fold=fold+1\n",
    "\n",
    "#print 10 fold's average f-score\n",
    "print(\"Average F-score across 10 folds is: \", np.mean(f1s))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

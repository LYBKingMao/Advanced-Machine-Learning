{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "lb=preprocessing.LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit label binarizer by classes \n",
    "lb.fit([0,1,2,3,4,5,6,7,8,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-processing\n",
    "data,label=datasets.load_digits(return_X_y=True);\n",
    "data = preprocessing.MinMaxScaler().fit_transform(data)\n",
    "label=label.reshape(len(label),1)\n",
    "\n",
    "#classes included in dataset\n",
    "labels = [0,1,2,3,4,5,6,7,8,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set number of fold, random seeds minibatch size and other variables\n",
    "kf=KFold(n_splits=10,shuffle=True)\n",
    "np.random.seed(0)\n",
    "minibatch_size=64\n",
    "l=0.1\n",
    "epochs=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relu function\n",
    "def ReLu(v):\n",
    "    return np.maximum(0,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#derivative of relu funtion\n",
    "def ReLu_derivative(v):\n",
    "    return np.where(v<=0,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LReLU(x):\n",
    "    return np.where(x > 0, x, x * 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dLReLU(x):\n",
    "    return np.where(x > 0, 1, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax funtion\n",
    "def softmax(X):\n",
    "    exps = np.exp(X)\n",
    "    return exps / np.sum(exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert softmax output to single label\n",
    "def convert(arr):\n",
    "    arr_size = arr.shape[1]\n",
    "    arr_max = np.argmax(arr, axis=1)\n",
    "    return arr_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error of fold 1 epoch 0 is: 1.624465482388124\n",
      "Training error of fold 1 epoch 100 is: 0.002823374381095505\n",
      "Training error of fold 1 epoch 200 is: 0.0014938806838334653\n",
      "Training error of fold 1 epoch 300 is: 0.0010203798516419595\n",
      "Training error of fold 1 epoch 400 is: 0.0007707933500399815\n",
      "Training error of fold 1 epoch 500 is: 0.0006327420772989597\n",
      "Training error of fold 1 epoch 600 is: 0.0005407299203627159\n",
      "Training error of fold 1 epoch 700 is: 0.00046600732237291563\n",
      "Training error of fold 1 epoch 800 is: 0.00041139301281883835\n",
      "Training error of fold 1 epoch 900 is: 0.00036865837322481884\n",
      "Fold 1 's f-score is: 0.9619747207373217\n",
      "Training error of fold 2 epoch 0 is: 1.585252659102538\n",
      "Training error of fold 2 epoch 100 is: 0.0017816989304043683\n",
      "Training error of fold 2 epoch 200 is: 0.0011782885211804968\n",
      "Training error of fold 2 epoch 300 is: 0.0009010243741863484\n",
      "Training error of fold 2 epoch 400 is: 0.000724612024938969\n",
      "Training error of fold 2 epoch 500 is: 0.0006059588476149042\n",
      "Training error of fold 2 epoch 600 is: 0.0005213309959041464\n",
      "Training error of fold 2 epoch 700 is: 0.00045957311616477725\n",
      "Training error of fold 2 epoch 800 is: 0.00040929457240257444\n",
      "Training error of fold 2 epoch 900 is: 0.0003675922930075616\n",
      "Fold 2 's f-score is: 0.9707755455985196\n",
      "Training error of fold 3 epoch 0 is: 3.9475371195795814\n",
      "Training error of fold 3 epoch 100 is: 0.0028950456260701113\n",
      "Training error of fold 3 epoch 200 is: 0.001672427632318595\n",
      "Training error of fold 3 epoch 300 is: 0.0012171256286723946\n",
      "Training error of fold 3 epoch 400 is: 0.0009473179567336339\n",
      "Training error of fold 3 epoch 500 is: 0.0007803575915731973\n",
      "Training error of fold 3 epoch 600 is: 0.0006545568337046249\n",
      "Training error of fold 3 epoch 700 is: 0.0005607388624906803\n",
      "Training error of fold 3 epoch 800 is: 0.0004926264067377444\n",
      "Training error of fold 3 epoch 900 is: 0.0004386978253342747\n",
      "Fold 3 's f-score is: 0.95598204339538\n",
      "Training error of fold 4 epoch 0 is: 0.8363897044588566\n",
      "Training error of fold 4 epoch 100 is: 0.003434536832504936\n",
      "Training error of fold 4 epoch 200 is: 0.0015658800441201842\n",
      "Training error of fold 4 epoch 300 is: 0.0010279554813294042\n",
      "Training error of fold 4 epoch 400 is: 0.0007546391096992851\n",
      "Training error of fold 4 epoch 500 is: 0.0005995144971431836\n",
      "Training error of fold 4 epoch 600 is: 0.000498152712972578\n",
      "Training error of fold 4 epoch 700 is: 0.00042603989781915574\n",
      "Training error of fold 4 epoch 800 is: 0.0003714634735743753\n",
      "Training error of fold 4 epoch 900 is: 0.00032943249715451206\n",
      "Fold 4 's f-score is: 0.969052748326942\n",
      "Training error of fold 5 epoch 0 is: 1.4878196156853256\n",
      "Training error of fold 5 epoch 100 is: 0.004306824648714092\n",
      "Training error of fold 5 epoch 200 is: 0.0017865113914913662\n",
      "Training error of fold 5 epoch 300 is: 0.0011144743036136438\n",
      "Training error of fold 5 epoch 400 is: 0.0008184893551719359\n",
      "Training error of fold 5 epoch 500 is: 0.0006478427553236086\n",
      "Training error of fold 5 epoch 600 is: 0.000530438296227097\n",
      "Training error of fold 5 epoch 700 is: 0.0004516117961933508\n",
      "Training error of fold 5 epoch 800 is: 0.0003911868060213042\n",
      "Training error of fold 5 epoch 900 is: 0.000345847832718261\n",
      "Fold 5 's f-score is: 0.981487827928506\n",
      "Training error of fold 6 epoch 0 is: 6.239216651378072\n",
      "Training error of fold 6 epoch 100 is: 0.0016924551887969102\n",
      "Training error of fold 6 epoch 200 is: 0.0010600912619683123\n",
      "Training error of fold 6 epoch 300 is: 0.0008020314917506309\n",
      "Training error of fold 6 epoch 400 is: 0.000645286544663826\n",
      "Training error of fold 6 epoch 500 is: 0.0005366241383940234\n",
      "Training error of fold 6 epoch 600 is: 0.0004580600400999494\n",
      "Training error of fold 6 epoch 700 is: 0.00039689383715438184\n",
      "Training error of fold 6 epoch 800 is: 0.00034787458856489297\n",
      "Training error of fold 6 epoch 900 is: 0.00030968465459870484\n",
      "Fold 6 's f-score is: 0.9639080254743406\n",
      "Training error of fold 7 epoch 0 is: 3.3781585695339267\n",
      "Training error of fold 7 epoch 100 is: 0.002675138956183965\n",
      "Training error of fold 7 epoch 200 is: 0.0014472589541206398\n",
      "Training error of fold 7 epoch 300 is: 0.0010055722027413868\n",
      "Training error of fold 7 epoch 400 is: 0.0007895862078198275\n",
      "Training error of fold 7 epoch 500 is: 0.0006476622585932385\n",
      "Training error of fold 7 epoch 600 is: 0.0005435797824847138\n",
      "Training error of fold 7 epoch 700 is: 0.0004727530370554922\n",
      "Training error of fold 7 epoch 800 is: 0.0004197447002795413\n",
      "Training error of fold 7 epoch 900 is: 0.000375371948481923\n",
      "Fold 7 's f-score is: 0.9721272362698509\n",
      "Training error of fold 8 epoch 0 is: 5.114163077453509\n",
      "Training error of fold 8 epoch 100 is: 0.005255941665495062\n",
      "Training error of fold 8 epoch 200 is: 0.002186481091356008\n",
      "Training error of fold 8 epoch 300 is: 0.0013629611030269572\n",
      "Training error of fold 8 epoch 400 is: 0.0009978793344477027\n",
      "Training error of fold 8 epoch 500 is: 0.0007911934458384238\n",
      "Training error of fold 8 epoch 600 is: 0.000650985990191467\n",
      "Training error of fold 8 epoch 700 is: 0.0005513944067684595\n",
      "Training error of fold 8 epoch 800 is: 0.00048093021647686147\n",
      "Training error of fold 8 epoch 900 is: 0.0004262789974144844\n",
      "Fold 8 's f-score is: 0.9748177174039243\n",
      "Training error of fold 9 epoch 0 is: 2.4707667486602163\n",
      "Training error of fold 9 epoch 100 is: 0.0005728937651653609\n",
      "Training error of fold 9 epoch 200 is: 0.00030788284929908054\n",
      "Training error of fold 9 epoch 300 is: 0.0002458701833127119\n",
      "Training error of fold 9 epoch 400 is: 0.00020848376886146958\n",
      "Training error of fold 9 epoch 500 is: 0.00018018120289692004\n",
      "Training error of fold 9 epoch 600 is: 0.00015834230143848532\n",
      "Training error of fold 9 epoch 700 is: 0.0001413716160194192\n",
      "Training error of fold 9 epoch 800 is: 0.00012804607351749386\n",
      "Training error of fold 9 epoch 900 is: 0.00011721022455108126\n",
      "Fold 9 's f-score is: 0.9669774811822617\n",
      "Training error of fold 10 epoch 0 is: 2.02699093244072\n",
      "Training error of fold 10 epoch 100 is: 0.003565115259552054\n",
      "Training error of fold 10 epoch 200 is: 0.0021087188214501795\n",
      "Training error of fold 10 epoch 300 is: 0.0014557039173896972\n",
      "Training error of fold 10 epoch 400 is: 0.0011034317713463696\n",
      "Training error of fold 10 epoch 500 is: 0.0008749795946798078\n",
      "Training error of fold 10 epoch 600 is: 0.0007293923676406187\n",
      "Training error of fold 10 epoch 700 is: 0.000623401050585625\n",
      "Training error of fold 10 epoch 800 is: 0.0005436804690920362\n",
      "Training error of fold 10 epoch 900 is: 0.0004823064800000673\n",
      "Fold 10 's f-score is: 0.981638382568615\n",
      "Average F-score across 10 folds is:  0.9698741728885663\n"
     ]
    }
   ],
   "source": [
    "#create an array to store each fold's f-score, create a varible to visualize current progress\n",
    "f1s=[]\n",
    "fold=1;\n",
    "\n",
    "#cross validation split\n",
    "for train_index,test_index in kf.split(data):\n",
    "    w = np.random.uniform(-1,1,[data.shape[1],128])        #10 neurons for hidden layer\n",
    "    b = np.zeros([1,128])                                       #number of bias equal to number of neurons in hidden layer\n",
    "    w2 = np.random.uniform(-1,1,[128,10])                       #10 output neurons for 10 classes\n",
    "    b2 = np.zeros([1,10])\n",
    "    \n",
    "    #epoch split\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        #minibatch split\n",
    "        for i in range(0,data[train_index].shape[0],minibatch_size):\n",
    "            x_train=data[train_index]\n",
    "            y_train=label[train_index]\n",
    "            x_mini=x_train[i:i+minibatch_size,:]\n",
    "            y_mini=y_train[i:i+minibatch_size,:]\n",
    "        \n",
    "            #forward pass\n",
    "            in_=x_mini@w+b\n",
    "            out1=ReLu(in_)\n",
    "            in2=out1@w2+b2\n",
    "            out2=np.zeros([x_mini.shape[0],10])\n",
    "            for j in range(x_mini.shape[0]):\n",
    "                out2[j,:]=softmax(in2[j,:])\n",
    "        \n",
    "            #one-hot label\n",
    "            oh_label=lb.transform(y_mini)\n",
    "            \n",
    "            #training error\n",
    "            train_error=metrics.log_loss(y_mini,out2,labels=labels)\n",
    "        \n",
    "            #Back propergation layer2\n",
    "            dEdIn2=np.zeros([x_mini.shape[0],10]) \n",
    "            for j in range(x_mini.shape[0]):\n",
    "                dEdIn2[j,:]=out2[j,:]-oh_label[j,:]\n",
    "            dIn2_dW2 = out1\n",
    "            dEdW2 = (1/x_mini.shape[0])*out1.T@dEdIn2\n",
    "            dEdB2 = (1/x_mini.shape[0])*np.ones([1,len(x_mini)])@dEdIn2\n",
    "            \n",
    "            #update layer2's weights and bias\n",
    "            w2 -= l*dEdW2\n",
    "            b2 -= l*dEdB2\n",
    "\n",
    "            #Back propergation layer1\n",
    "            dEdOut1 = dEdIn2 @ w2.T\n",
    "            dOut1dIn1 = ReLu_derivative(in_)\n",
    "            dEdIn1 = dEdOut1*dOut1dIn1\n",
    "            dIn1dW = x_mini\n",
    "            dEdW = (1/x_mini.shape[0])*dIn1dW.T@((dEdIn2@w2.T)*dOut1dIn1)\n",
    "            dEdB = (1/x_mini.shape[0])*np.ones([len(x_mini)])@((dEdIn2@w2.T)*dOut1dIn1)\n",
    "\n",
    "            #update layer1's weights and bias\n",
    "            w -= l*dEdW\n",
    "            b -= l*dEdB\n",
    "        \n",
    "        #print output for each 100 epoch\n",
    "        if(epoch%100==0):\n",
    "            print(\"Training error of fold\",fold,\"epoch\",epoch,\"is:\",train_error)\n",
    "            \n",
    "    #predict test dataset and convert result to label         \n",
    "    in22=(ReLu(data[test_index]@w+b)@w2)+b2\n",
    "    out22=np.zeros([data[test_index].shape[0],10])\n",
    "    for j in range(data[test_index].shape[0]):\n",
    "        out22[j,:]=softmax(in22[j,:])\n",
    "    prediction=convert(out22)\n",
    "    \n",
    "    #calculate each fold's f-score\n",
    "    f1=f1_score(y_true=label[test_index], y_pred=prediction, labels=labels,average='macro')\n",
    "    print(\"Fold\",fold,\"'s f-score is:\",f1)\n",
    "    f1s.append(f1)\n",
    "    fold=fold+1\n",
    "\n",
    "#print 10 fold's average f-score\n",
    "print(\"Average F-score across 10 folds is: \", np.mean(f1s))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

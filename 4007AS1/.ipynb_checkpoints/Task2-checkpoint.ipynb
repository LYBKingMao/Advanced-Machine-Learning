{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "lb=preprocessing.LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit label binarizer by classes \n",
    "lb.fit([0,1,2,3,4,5,6,7,8,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-processing\n",
    "data,label=datasets.load_digits(return_X_y=True);\n",
    "data = preprocessing.MinMaxScaler().fit_transform(data)\n",
    "label=label.reshape(len(label),1)\n",
    "\n",
    "#classes included in dataset\n",
    "labels = [0,1,2,3,4,5,6,7,8,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set number of fold, random seeds minibatch size and other variables\n",
    "kf=KFold(n_splits=10,shuffle=True)\n",
    "np.random.seed(0)\n",
    "minibatch_size=128\n",
    "l=0.01\n",
    "epochs=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relu function\n",
    "def ReLu(v):\n",
    "    return np.maximum(0,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#derivative of relu funtion\n",
    "def ReLu_derivative(v):\n",
    "    return np.where(v<=0,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax funtion\n",
    "def softmax(X):\n",
    "    exps = np.exp(X)\n",
    "    return exps / np.sum(exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert softmax output to single label\n",
    "def convert(arr):\n",
    "    arr_size = arr.shape[1]\n",
    "    arr_max = np.argmax(arr, axis=1)\n",
    "    return arr_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error of fold 1 epoch 0 is: 6.077740559982455\n",
      "Training error of fold 1 epoch 100 is: 1.0594304190454154\n",
      "Training error of fold 1 epoch 200 is: 0.6194675510749282\n",
      "Training error of fold 1 epoch 300 is: 0.4177586361026791\n",
      "Training error of fold 1 epoch 400 is: 0.31837559846245406\n",
      "Fold 1 's f-score is: 0.8973113056336672\n",
      "Training error of fold 2 epoch 0 is: 4.730870697892228\n",
      "Training error of fold 2 epoch 100 is: 0.5822308047842655\n",
      "Training error of fold 2 epoch 200 is: 0.317307547463822\n",
      "Training error of fold 2 epoch 300 is: 0.24398369073080461\n",
      "Training error of fold 2 epoch 400 is: 0.20093963401118486\n",
      "Fold 2 's f-score is: 0.9339063410925121\n",
      "Training error of fold 3 epoch 0 is: 4.862286853745861\n",
      "Training error of fold 3 epoch 100 is: 1.068523774805214\n",
      "Training error of fold 3 epoch 200 is: 0.4983904145978191\n",
      "Training error of fold 3 epoch 300 is: 0.3382123281431242\n",
      "Training error of fold 3 epoch 400 is: 0.27025811656532395\n",
      "Fold 3 's f-score is: 0.9204012380501869\n",
      "Training error of fold 4 epoch 0 is: 4.241345136733649\n",
      "Training error of fold 4 epoch 100 is: 1.0483175948151502\n",
      "Training error of fold 4 epoch 200 is: 0.6368537565335702\n",
      "Training error of fold 4 epoch 300 is: 0.45966875957391945\n",
      "Training error of fold 4 epoch 400 is: 0.37290699039544506\n",
      "Fold 4 's f-score is: 0.8977879211193811\n",
      "Training error of fold 5 epoch 0 is: 3.1629444735894285\n",
      "Training error of fold 5 epoch 100 is: 1.0555945417075705\n",
      "Training error of fold 5 epoch 200 is: 0.6107063025694747\n",
      "Training error of fold 5 epoch 300 is: 0.42090132473991954\n",
      "Training error of fold 5 epoch 400 is: 0.3364338808504123\n",
      "Fold 5 's f-score is: 0.9269771916283546\n",
      "Training error of fold 6 epoch 0 is: 4.2077820969113295\n",
      "Training error of fold 6 epoch 100 is: 1.0823367869634555\n",
      "Training error of fold 6 epoch 200 is: 0.5255940504531251\n",
      "Training error of fold 6 epoch 300 is: 0.3330653871743918\n",
      "Training error of fold 6 epoch 400 is: 0.247296426367889\n",
      "Fold 6 's f-score is: 0.9053912915346801\n",
      "Training error of fold 7 epoch 0 is: 4.098104708967188\n",
      "Training error of fold 7 epoch 100 is: 1.5276457863453758\n",
      "Training error of fold 7 epoch 200 is: 0.6405222402194652\n",
      "Training error of fold 7 epoch 300 is: 0.40547220942744294\n",
      "Training error of fold 7 epoch 400 is: 0.31270478751790365\n",
      "Fold 7 's f-score is: 0.8926329296445645\n",
      "Training error of fold 8 epoch 0 is: 3.706689434784735\n",
      "Training error of fold 8 epoch 100 is: 0.8043388639967525\n",
      "Training error of fold 8 epoch 200 is: 0.41871834413884174\n",
      "Training error of fold 8 epoch 300 is: 0.2974351646870324\n",
      "Training error of fold 8 epoch 400 is: 0.24252555996247632\n",
      "Fold 8 's f-score is: 0.944541481909903\n",
      "Training error of fold 9 epoch 0 is: 3.3378536180874243\n",
      "Training error of fold 9 epoch 100 is: 1.0335712596848208\n",
      "Training error of fold 9 epoch 200 is: 0.6485316573496713\n",
      "Training error of fold 9 epoch 300 is: 0.5154714467774395\n",
      "Training error of fold 9 epoch 400 is: 0.4290292886713275\n",
      "Fold 9 's f-score is: 0.8198717627401839\n",
      "Training error of fold 10 epoch 0 is: 3.1221874023226395\n",
      "Training error of fold 10 epoch 100 is: 1.3270108205707538\n",
      "Training error of fold 10 epoch 200 is: 0.8640324678526551\n",
      "Training error of fold 10 epoch 300 is: 0.6597223810818137\n",
      "Training error of fold 10 epoch 400 is: 0.5083393227152281\n",
      "Fold 10 's f-score is: 0.8301318435606362\n",
      "Average F-score across 10 folds is:  0.896895330691407\n"
     ]
    }
   ],
   "source": [
    "#create an array to store each fold's f-score, create a varible to visualize current progress\n",
    "f1s=[]\n",
    "fold=1;\n",
    "\n",
    "#cross validation split\n",
    "for train_index,test_index in kf.split(data):\n",
    "    w = np.random.uniform(-1,1,[data.shape[1],10])        #10 neurons for hidden layer\n",
    "    b = np.zeros([1,10])                                       #number of bias equal to number of neurons in hidden layer\n",
    "    w2 = np.random.uniform(-1,1,[10,10])                       #10 output neurons for 10 classes\n",
    "    b2 = np.zeros([1,10])\n",
    "    \n",
    "    #epoch split\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        #minibatch split\n",
    "        for i in range(0,data[train_index].shape[0],minibatch_size):\n",
    "            x_train=data[train_index]\n",
    "            y_train=label[train_index]\n",
    "            x_mini=x_train[i:i+minibatch_size,:]\n",
    "            y_mini=y_train[i:i+minibatch_size,:]\n",
    "        \n",
    "            #forward pass\n",
    "            in_=x_mini@w+b\n",
    "            out1=ReLu(in_)\n",
    "            in2=out1@w2+b2\n",
    "            out2=np.zeros([x_mini.shape[0],10])\n",
    "            for j in range(x_mini.shape[0]):\n",
    "                out2[j,:]=softmax(in2[j,:])\n",
    "        \n",
    "            #one-hot label\n",
    "            oh_label=lb.transform(y_mini)\n",
    "            \n",
    "            #training error\n",
    "            train_error=metrics.log_loss(y_mini,out2,labels=labels)\n",
    "        \n",
    "            #Back propergation layer2\n",
    "            dEdIn2=np.zeros([x_mini.shape[0],10]) \n",
    "            for j in range(x_mini.shape[0]):\n",
    "                dEdIn2[j,:]=out2[j,:]-oh_label[j,:]\n",
    "            dIn2_dW2 = out1\n",
    "            dEdW2 = (1/x_mini.shape[0])*out1.T@dEdIn2\n",
    "            dEdB2 = (1/x_mini.shape[0])*np.ones([1,len(x_mini)])@dEdIn2\n",
    "            \n",
    "            #update layer2's weights and bias\n",
    "            w2 -= l*dEdW2\n",
    "            b2 -= l*dEdB2\n",
    "\n",
    "            #Back propergation layer1\n",
    "            dEdOut1 = dEdIn2 @ w2.T\n",
    "            dOut1dIn1 = ReLu_derivative(in_)\n",
    "            dEdIn1 = dEdOut1*dOut1dIn1\n",
    "            dIn1dW = x_mini\n",
    "            dEdW = (1/x_mini.shape[0])*dIn1dW.T@((dEdIn2@w2.T)*dOut1dIn1)\n",
    "            dEdB = (1/x_mini.shape[0])*np.ones([len(x_mini)])@((dEdIn2@w2.T)*dOut1dIn1)\n",
    "\n",
    "            #update layer1's weights and bias\n",
    "            w -= l*dEdW\n",
    "            b -= l*dEdB\n",
    "        \n",
    "        #print output for each 100 epoch\n",
    "        if(epoch%100==0):\n",
    "            print(\"Training error of fold\",fold,\"epoch\",epoch,\"is:\",train_error)\n",
    "            \n",
    "    #predict test dataset and convert result to label         \n",
    "    in22=(ReLu(data[test_index]@w+b)@w2)+b2\n",
    "    out22=np.zeros([data[test_index].shape[0],10])\n",
    "    for j in range(data[test_index].shape[0]):\n",
    "        out22[j,:]=softmax(in22[j,:])\n",
    "    prediction=convert(out22)\n",
    "    \n",
    "    #calculate each fold's f-score\n",
    "    f1=f1_score(y_true=label[test_index], y_pred=prediction, labels=labels,average='macro')\n",
    "    print(\"Fold\",fold,\"'s f-score is:\",f1)\n",
    "    f1s.append(f1)\n",
    "    fold=fold+1\n",
    "\n",
    "#print 10 fold's average f-score\n",
    "print(\"Average F-score across 10 folds is: \", np.mean(f1s))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
